We might have a practical way to make long-context reasoning actually work for agents.Most people still frame “long context” as a token problem: bigger window, paste more text, hope the model keeps up. But we’ve known for a while that raw prompting deteriorates as inputs get longer.The MIT CSAIL Recursive Language Models (RLM) paper rubber-stamps a better frame: there are two scaling axes:Length: how many tokens existComplexity (information density): how much of the artefact you must ingest to answer correctlyThat second axis is what kills agents in legal and large-code work.A massive FAQ can be long but low density. Each question is local. RAG works because semantic similarity is a decent proxy for relevance.A merger agreement or a large codebase is different. The answer is often non-local and hidden behind cross-references (see Section X, exceptions in Y, definitions in Z, functions calling functions). RAG fails because similarity retrieval can fetch the “right-looking” clause, but miss the clauses it depends on, since dependencies are structural, not semantically similar.RLM’s move is simple: stop treating the document as “tokens for attention”.Put the artefact into a Python REPL (Read-Eval-Print Loop), basically an interactive Python console. Store the text as a variable. Let the agent write code to index, search, follow references, and verify. Then add recursion: sub-calls chase specific branches of the dependency graph and return evidence, building a tree instead of one giant prompt.The paper shows this can handle inputs up to two orders of magnitude beyond the base model’s context window, with comparable or sometimes lower cost per query than common long-context scaffolds.Rule of thumb I’m adopting:RAG when answers are localREPL loop when you need iterative inspection + verificationREPL + recursion when dependency depth dominates (contracts, repos, multi-doc diligence)I did a live run-through of the paper. I'll link in the comments.